# 一、语言模型



## **N元文法与模型应用**

定义：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211129172219172.png" alt="image-20211129172219172" style="zoom: 33%;" />

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211129175735265.png" alt="image-20211129175735265" style="zoom:33%;" />

例如，对于三元文法，例句“今天是个灿烂的日子”：

<BOS> 今天 是，今天 是 个，是 个 灿烂，个 灿烂 的，灿烂 的 日子，的 日子  <EOS>



## 

## 神经网络中的神经元概述

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201125820269.png" alt="image-20211201125820269" style="zoom: 33%;" />

#### 神经网络搭建

各参数的定义：

其中，x一般称为输入层，y一般称为输出层，h一般称为隐层

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201161014851.png" alt="image-20211201161014851" style="zoom:33%;" />

参数的计算：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201161209176.png" alt="image-20211201161209176" style="zoom:33%;" />

这里的hj=每个指向j的x的值与箭头上对应的u的值相乘的和，引出问题，这么相加起来的值依旧是线性的，在一些线性不可分的情况下怎么办？

因此引出非线性激活函数：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201164136389.png" alt="image-20211201164136389" style="zoom:33%;" />

将这个非线性激活函数设为f()，令:

​                      hj = f(每个指向j的x的值与箭头上对应的u的值相乘的和)

同时：

​						y i = f(每个指向y的h的值与箭头上对应的w的值相乘的和)

#### 案例1

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201164740858.png" alt="image-20211201164740858" style="zoom:25%;" />

这里设定：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201165156429.png" alt="image-20211201165156429" style="zoom:33%;" />

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201171322084.png" alt="image-20211201171322084" style="zoom:33%;" />

每个箭头的u值和w值如上图，其中X2相当于wx+b中的b，h2同理。

X1=1或0，X0 =1或 0，也就是说X1与X0相当于异或的计算的输入，对于异或运算，X1与X2相同时是0，不同时是1。

那么可以推算出如下的结果：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201175452495.png" alt="image-20211201175452495" style="zoom:33%;" />

对于输出节点y0，当输入(0，0)时y0应该等于0，(0，1)应该等于1，(1，0)等输入同理，也就是说，一开始我们定义的u与w的值所算出来的y0并不与**预期值**相同，这可怎么办呢？

#### 因此引出**[反向传播](https://blog.csdn.net/ft_sunshine/article/details/90221691)**：

首先定义如下：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201183432672.png" alt="image-20211201183432672" style="zoom:33%;" />

上图中ti是预期值也就是目标输出，E所计算的是平方误差。

我们要对w也就是h到y这一层的权重求偏导，求导步骤如下：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201191257734.png" alt="image-20211201191257734" style="zoom: 33%;" />

上式使用链式求导法则，求出j到i之间的w差值，其中：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201192246533.png" alt="image-20211201192246533" style="zoom:33%;" />

最后得到：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201192506615.png" alt="image-20211201192506615" style="zoom:33%;" />

接着求E对u的偏导：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201192905789.png" alt="image-20211201192905789" style="zoom:33%;" />

最终结果算为：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201193054641.png" alt="image-20211201193054641" style="zoom:33%;" />

其中：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201193119437.png" alt="image-20211201193119437" style="zoom:33%;" />

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201194332871.png" alt="image-20211201194332871" style="zoom:33%;" />

#### 案例2

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201194523407.png" alt="image-20211201194523407" style="zoom:33%;" />

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201194538207.png" alt="image-20211201194538207" style="zoom:33%;" />

其中，输入与目标输出为：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201194821574.png" alt="image-20211201194821574" style="zoom:33%;" />

计算过程如下：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201202250870.png" alt="image-20211201202250870" style="zoom:33%;" />

1：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201204021221.png" alt="image-20211201204021221" style="zoom:33%;" />

2：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201204119087.png" alt="image-20211201204119087" style="zoom:33%;" />

3：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201205649268.png" alt="image-20211201205649268" style="zoom: 50%;" />

4：

算出这些变化值后，将这些值乘以学习速率η，记为k，一般来说新的权重值等于原权重减去k



## **前馈神经网络（FNN）语言模型**

在前馈神经网络中，各神经元分别属于不同的层。每一层的神经元可以接收前一层神经元的信号，并产生信号输出到下一层。第0层叫输入层，最后一层叫输出层，其它中间层叫做隐藏层。整个网络中无反馈，信号从输入层向输出层单向传播。又被称为**多层感知机(Multi-Layer Perceptron,MLP)**。



#### 问题引入：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201221623544.png" alt="image-20211201221623544" style="zoom: 25%;" />

当出现“很”的时候，下文出现风趣和幽默这两种意思相近的词n-gram往往效果表现的很差，那么使用神经网络的方法去训练模型该是什么样的效果呢？由怎样去训练模型呢？



#### 词向量的表示：

使用自查表(look up table)

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201223331546.png" alt="image-20211201223331546" style="zoom:33%;" />

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201223433438.png" alt="image-20211201223433438" style="zoom: 30%;" />

举个例子：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201224243322.png" alt="image-20211201224243322" style="zoom:33%;" />

V×D = 5000 × 2

#### softmax回归：

​	Softmax 回归也称为多项(Multinomial)或多类(Multi-Class)的 Logistic 回归, 是Logistic 回归在多分类问题上的推广，它也可 以看作是一种条件最大熵模型。对于多类问题，类别标签 yÎ{1, 2, ⋯ ,C}可以有C个取值， 给定一个样本x, Softmax 回归预测的属于类别c 的条件概率为:

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201221407699.png" alt="image-20211201221407699" style="zoom:33%;" />

#### 案例

问题：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201224442026.png" alt="image-20211201224442026" style="zoom:33%;" />

求该出现乏味的概率？

整体框架：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201225222493.png" alt="image-20211201225222493" style="zoom:33%;" />

具体计算：

（1）查词表：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201225336714.png" alt="image-20211201225336714" style="zoom:33%;" />

（2）拼接所有的条件词的向量,形成一个向量：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201225403424.png" alt="image-20211201225403424" style="zoom:33%;" />

（3）隐藏层: 线性映射+非线性变换。

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201230002155.png" alt="image-20211201230002155" style="zoom:33%;" />

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201230232468.png" alt="image-20211201230232468" style="zoom:30%;" />

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201230254066.png" alt="image-20211201230254066" style="zoom:30%;" />

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201230823214.png" alt="image-20211201230823214" style="zoom:33%;" />

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211201230330124.png" alt="image-20211201230330124" style="zoom:33%;" />

可以看出乏味对应的是0.190，也就是再出现“这本书很”的情况下出现乏味的概率是0.190



## [循环神经网络语言模型](https://zhuanlan.zhihu.com/p/123211148)



**循环神经网络**（Recurrent neural network：**RNN**）是神经网络的一种。单纯的RNN因为无法处理随着递归，权重指数级爆炸或[梯度消失问题](https://zh.wikipedia.org/wiki/梯度消失问题)，难以捕捉长期时间关联；而结合不同的**[LSTM](https://zh.wikipedia.org/wiki/LSTM)**可以很好解决这个问题。

#### 网络架构

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211202112205882.png" alt="image-20211202112205882" style="zoom: 30%;" />

从左往右

1、当前位置的历史积累ht = tanh（当前输入的词wt乘以权重U + 权重W乘以ht-1）

2、词表Lt乘以当前位置的历史积累ht

3、输出到y的序列中成为其中一个值



#### 反向传播

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211202120809761.png" alt="image-20211202120809761" style="zoom: 30%;" />

#### 案例：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211202120919840.png" alt="image-20211202120919840" style="zoom:30%;" />

（1）h0为t=0时的值，一般初始化为0。

（2）U与W都设为<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211202121403938.png" alt="image-20211202121403938" style="zoom:33%;" />

接着，调用softmax函数<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211202121555913.png" alt="image-20211202121555913" style="zoom: 33%;" />计算概率，其中，**语言模型**是对下一个词出现的**概率**进行建模。那么，怎样让神经网络输出概率呢？方法就是用softmax层作为神经网络的输出层。

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211202121501819.png" alt="image-20211202121501819" style="zoom:33%;" />



引出问题：

<img src="C:\Users\WELKIN\AppData\Roaming\Typora\typora-user-images\image-20211205231324715.png" alt="image-20211205231324715" style="zoom:33%;" />

LSTM的介绍见[这篇](https://zhuanlan.zhihu.com/p/32085405)：

<img src="https://pic2.zhimg.com/v2-556c74f0e025a47fea05dc0f76ea775d_1440w.jpg?source=172ae18b" alt="人人都能看懂的LSTM" style="zoom:33%;" />



## 自注意力神经语言模型

主要介绍自我注意机制语言模型GPT（Language Model with Generative Pre-training），后面的课程会介绍到

